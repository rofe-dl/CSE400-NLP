{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from process_dataset import speech_features, text_features\n",
    "import numpy as np\n",
    "\n",
    "from ensemble import StackEnsemble, VoteEnsemble, BlendEnsemble\n",
    "\n",
    "from speech_models import speech_logistic_regression, speech_mlp, speech_naive_bayes, speech_random_forest, speech_svm, speech_xgboost\n",
    "from text_models import text_logistic_regression, text_mlp, text_naive_bayes, text_random_forest, text_svm, text_xgboost\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech_models():\n",
    "\n",
    "    models = list()\n",
    "\n",
    "    models.append(('Support Vector Machine', speech_svm.get_svm()))\n",
    "    models.append(('Random Forest Classifier', speech_random_forest.get_random_forest()))\n",
    "    models.append(('Multinomial Naive Bayes', speech_naive_bayes.get_naive_bayes()))\n",
    "    models.append(('Logistic Regression', speech_logistic_regression.get_logistic_regression()))\n",
    "    models.append(('MLP Classifier', speech_mlp.get_mlp()))\n",
    "    models.append(('XGBoost', speech_xgboost.get_xgb()))\n",
    "\n",
    "    # TODO lstm\n",
    "\n",
    "    return models\n",
    "\n",
    "def get_text_models():\n",
    "    \n",
    "    models = list()\n",
    "\n",
    "    models.append(('Support Vector Machine', text_svm.get_svm()))\n",
    "    models.append(('Random Forest Classifier', text_random_forest.get_random_forest()))\n",
    "    models.append(('Multinomial Naive Bayes', text_naive_bayes.get_naive_bayes()))\n",
    "    models.append(('Logistic Regression', text_logistic_regression.get_logistic_regression()))\n",
    "    models.append(('MLP Classifier', text_mlp.get_mlp()))\n",
    "    models.append(('XGBoost', text_xgboost.get_xgb()))\n",
    "\n",
    "    # TODO lstm \n",
    "\n",
    "    return models\n",
    "\n",
    "def print_scores(scores):\n",
    "    print('Accuracy: ', np.mean(scores['test_accuracy']))\n",
    "    print('F1 Macro: ', np.mean(scores['test_f1_macro']))\n",
    "    print('Precision Macro: ', np.mean(scores['test_precision_macro']))\n",
    "    print('Recall Macro: ', np.mean(scores['test_recall_macro']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_s, x_test_s, y_train_s, y_test_s = speech_features.get_train_test()\n",
    "x_train_t, x_test_t, y_train_t, y_test_t = text_features.get_train_test()\n",
    "speech_x_y = speech_features.get_data()\n",
    "text_x_y = text_features.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vote Ensemble (Soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter = VoteEnsemble(get_speech_models(), get_text_models(), type='soft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter.fit(x_train_s, x_train_t, y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang     0.8983    0.7644    0.8260       208\n",
      "         hap     0.8019    0.7792    0.7904       317\n",
      "         neu     0.7342    0.7561    0.7450       369\n",
      "         sad     0.7273    0.8263    0.7736       213\n",
      "\n",
      "    accuracy                         0.7778      1107\n",
      "   macro avg     0.7904    0.7815    0.7837      1107\n",
      "weighted avg     0.7831    0.7778    0.7787      1107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_result = voter.predict(x_test_s, x_test_t)\n",
    "print(classification_report(y_test_s, voter_result, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter.save('soft_voter.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Fold Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_k_fold = voter.cross_validate(speech_x_y[0], text_x_y[0], speech_x_y[1], cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(voter_k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vote Ensemble (Hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter2 = VoteEnsemble(get_speech_models(), get_text_models(), type='hard')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Support Vector Machine (Speech) ...\n",
      "Training Random Forest Classifier (Speech) ...\n",
      "Training Multinomial Naive Bayes (Speech) ...\n",
      "Training Logistic Regression (Speech) ...\n",
      "Training MLP Classifier (Speech) ...\n",
      "Training XGBoost (Speech) ...\n",
      "[04:18:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training Support Vector Machine (Text) ...\n",
      "Training Random Forest Classifier (Text) ...\n",
      "Training Multinomial Naive Bayes (Text) ...\n",
      "Training Logistic Regression (Text) ...\n",
      "Training MLP Classifier (Text) ...\n",
      "Training XGBoost (Text) ...\n",
      "[04:20:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"tree_methods\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[04:20:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "voter2.fit(x_train_s, x_train_t, y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang     0.8736    0.7308    0.7958       208\n",
      "         hap     0.7869    0.7224    0.7533       317\n",
      "         neu     0.6830    0.7534    0.7165       369\n",
      "         sad     0.7064    0.7793    0.7411       213\n",
      "\n",
      "    accuracy                         0.7453      1107\n",
      "   macro avg     0.7625    0.7465    0.7517      1107\n",
      "weighted avg     0.7531    0.7453    0.7467      1107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_result2 = voter2.predict(x_test_s, x_test_t)\n",
    "print(classification_report(y_test_s, voter_result2, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter2.save('hard_voter.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Fold Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_k_fold2 = voter2.cross_validate(speech_x_y[0], text_x_y[0], speech_x_y[1], cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(voter_k_fold2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blend Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cls = LogisticRegression(solver='liblinear', random_state=42)\n",
    "blender = BlendEnsemble(get_speech_models(), get_text_models(), meta_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Support Vector Machine (Speech) ...\n",
      "Training Random Forest Classifier (Speech) ...\n",
      "Training Multinomial Naive Bayes (Speech) ...\n",
      "Training Logistic Regression (Speech) ...\n",
      "Training MLP Classifier (Speech) ...\n",
      "Training XGBoost (Speech) ...\n",
      "[04:25:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training Support Vector Machine (Text) ...\n",
      "Training Random Forest Classifier (Text) ...\n",
      "Training Multinomial Naive Bayes (Text) ...\n",
      "Training Logistic Regression (Text) ...\n",
      "Training MLP Classifier (Text) ...\n",
      "Training XGBoost (Text) ...\n",
      "[04:26:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"tree_methods\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[04:26:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training Meta Classifier ...\n"
     ]
    }
   ],
   "source": [
    "blender.fit(x_train_s, x_train_t, y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang     0.8333    0.8173    0.8252       208\n",
      "         hap     0.7699    0.7918    0.7807       317\n",
      "         neu     0.7774    0.6911    0.7317       369\n",
      "         sad     0.7269    0.8498    0.7835       213\n",
      "\n",
      "    accuracy                         0.7742      1107\n",
      "   macro avg     0.7769    0.7875    0.7803      1107\n",
      "weighted avg     0.7761    0.7742    0.7733      1107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blender_result = blender.predict(x_test_s, x_test_t)\n",
    "print(classification_report(y_test_s, blender_result, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender.save('blender.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Fold Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender_k_fold = blender.cross_validate(speech_x_y[0], text_x_y[0], speech_x_y[1], cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(blender_k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cls2 = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "stacker = StackEnsemble(get_speech_models(), get_text_models(), meta_cls2, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stacker.fit(x_train_s, x_train_t, y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang     0.8122    0.8317    0.8219       208\n",
      "         hap     0.8072    0.7792    0.7929       317\n",
      "         neu     0.7695    0.7236    0.7458       369\n",
      "         sad     0.7386    0.8357    0.7841       213\n",
      "\n",
      "    accuracy                         0.7814      1107\n",
      "   macro avg     0.7819    0.7925    0.7862      1107\n",
      "weighted avg     0.7824    0.7814    0.7810      1107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stacker_result = stacker.predict(x_test_s, x_test_t)\n",
    "print(classification_report(y_test_s, stacker_result, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacker.save('stacker.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Fold Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacker_k_fold = stacker.cross_validate(speech_x_y[0], text_x_y[0], speech_x_y[1], cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(stacker_k_fold)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ed4bbf83b94bed540e4a6248a5de1e2eb8a809c16f0f6392d7883b316929957"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
